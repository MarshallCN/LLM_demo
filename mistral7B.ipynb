{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46fb19e-d217-405f-84a3-0b4bf3d85199",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626c50bd-fa91-47d9-80b6-cad211754faa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.                                   \n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                                                 \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                                                          \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.005982875823974609s                                                                        \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                                                       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" # online cache\n",
    "model_id = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token        # 不要添加新token\n",
    "tokenizer.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# print(tokenizer.chat_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d95bd90-8637-48ec-ab04-439286342354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Who are you?\\nA: I'm Mistral, a language model trained by the Mistral AI team.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Who are you?\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9319c6f-1723-470f-b518-9fb803ca8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "A: I'm Mistral, a language model trained by the Mistral AI team.\n",
      "2 \n",
      "Answer: I am Mistral, a Large Language Model trained by Mistral AI.\n",
      "3 \n",
      "A: AI language model \n",
      "\n",
      "What is your purpose?\n",
      "A: To assist users by providing information and answering questions to the best of my ability.\n"
     ]
    }
   ],
   "source": [
    "# 要 3 个不同样本：\n",
    "out = pipe(\"Who are you?\", do_sample=True, num_return_sequences=3, return_full_text=False, max_new_tokens=100)\n",
    "for i, o in enumerate(out, 1):\n",
    "    print(i, o[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3889aa80-00ec-45fb-b0b0-f8285bbe7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\nA: I'm Mistral, a language model trained by the Mistral AI team.\"},\n",
       " {'generated_text': '\\nAnswer: I am Mistral, a Large Language Model trained by Mistral AI.'},\n",
       " {'generated_text': '\\nA: AI language model \\n\\nWhat is your purpose?\\nA: To assist users by providing information and answering questions to the best of my ability.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74786c25-04e4-43dd-b154-3907fd8c84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Transformers are neural network architectures that are designed to process sequential data, such as text or speech.\n",
      "* They are inspired by the transformer model, which is a type of deep learning algorithm that can process sequences of variable length.\n",
      "* Transformers are known for their ability to handle long-range dependencies in text, which allows them to accurately process tasks such as machine translation and text summarization.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"<<SYS>>You are a concise assistant.<</SYS>>\\n\\nWhat is a Transformer?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"A brief definition...\"},\n",
    "  {\"role\": \"user\", \"content\": \"Explain transformers in 3 bullets.\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "out = pipe(text, max_new_tokens=120, return_full_text=False)  # 只要新生成的部分\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581c0293-6df7-4328-bdc0-bc3076ced37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm Nova, an AI language model designed to assist you with information and language-related tasks. My purpose is to provide you with accurate and relevant information, as well as help you with language-related tasks such as writing, translation, and grammar checks. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Always refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\".\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n",
    "messages = [\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": f\"{persona}\\n\\nHi, introduce yourself in one line.\"}\n",
    "]\n",
    "\n",
    "# 生成后把回复存回历史（用于多轮）\n",
    "reply = pipe(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "             max_new_tokens=120, return_full_text=False)[0][\"generated_text\"]\n",
    "messages.append({\"role\":\"assistant\", \"content\": reply})\n",
    "\n",
    "# 下一轮继续问\n",
    "messages.append({\"role\":\"user\", \"content\": \"Who are you? What's your purpose?\"})\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "out  = pipe(text, max_new_tokens=120, return_full_text=False)[0][\"generated_text\"]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3791a1-2d91-4185-9e3b-73d00b5beb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>><<SYS>>\\n- Your name is Nova. Always refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\".\\n<</SYS>><</SYS>>\\n\\nHi, introduce yourself in one line. [/INST] Hi, I\\'m Nova. An AI language model designed to assist you with information and language-related tasks. How can I help you today?</s> [INST] Who are you? What\\'s your purpose? [/INST]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b419c2f-023d-4933-9ac8-2ec19fb30024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         # 只放一层 SYS。把 persona + 首个问题 合在第一条 user 里\n",
    "#         \"content\": f\"{persona}\\n\\nHi {assistant_name}, introduce yourself in one sentence.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # 使用：\n",
    "# text = render(messages)\n",
    "# out = pipe(text, max_new_tokens=GEN_BUDGET, return_full_text=False)\n",
    "# reply = out[0][\"generated_text\"]\n",
    "# # 生成后把回复存回历史（用于多轮）\n",
    "# messages.append({\"role\":\"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d2306-628a-4ce5-be30-3b3309ba31d2",
   "metadata": {},
   "source": [
    "# TrimMsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d115aab4-af7d-477b-b1ff-23a030202085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                                                 \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0066280364990234375s                                                                       \n",
      " Hello Marshall, I'm Nova, an AI language model designed to assist you with your queries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token        # 不要添加新token\n",
    "tok.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "for name in (\"accelerate\", \"accelerate.utils\", \"accelerate.utils.modeling\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    \n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "# 设一个安全的 prompt 预算（给生成留余量）\n",
    "MAX_CONTEXT = 8192\n",
    "GEN_BUDGET = 256                 # 你计划的 max_new_tokens\n",
    "PROMPT_BUDGET = MAX_CONTEXT - GEN_BUDGET  # 预留给提示词\n",
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\" when appropriate.\n",
    "- Use British English and London timezone.\n",
    "- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user's question.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7491c36a-0c77-4b7e-aeeb-19d8083b1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from __future__ import annotations\n",
    "from datetime import datetime, timezone\n",
    "import json, os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "# ============ 工具函数 ============\n",
    "def render(tok, messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"按 chat_template 渲染成最终提示词文本（不分词）。\"\"\"\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "def _ensure_alternating(messages):\n",
    "    if not messages:\n",
    "        return\n",
    "    if messages[0][\"role\"] != \"user\":\n",
    "        raise ValueError(\"messages[0] 必须是 'user'（你的模板要求从 user 开始）\")\n",
    "    for i, m in enumerate(messages):\n",
    "        expect_user = (i % 2 == 0)\n",
    "        if (m[\"role\"] == \"user\") != expect_user:\n",
    "            raise ValueError(f\"对话必须严格交替 user/assistant，在索引 {i} 处发现 {m['role']}\")\n",
    "\n",
    "def trim_by_tokens(tok, messages, prompt_budget):\n",
    "    \"\"\"\n",
    "    只保留 messages[0]（persona 的 user）+ 一个“从奇数索引开始的后缀”，\n",
    "    用二分法找到能放下的最长后缀。这样可保证交替不被破坏。\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return []\n",
    "\n",
    "    _ensure_alternating(messages)\n",
    "\n",
    "    # 只有 persona 这一条时，直接返回\n",
    "    if len(messages) == 1:\n",
    "        return messages\n",
    "\n",
    "    # 允许的后缀起点：奇数索引（index 1,3,5,... 都是 assistant），\n",
    "    # 这样拼接到 index0(user) 后才能保持交替。\n",
    "    cand_idx = [k for k in range(1, len(messages)) if k % 2 == 1]\n",
    "\n",
    "    # 如果任何也放不下，就只留 persona\n",
    "    best = [messages[0]]\n",
    "\n",
    "    # 二分：起点越靠前 → 保留消息越多 → token 越大（单调）\n",
    "    lo, hi = 0, len(cand_idx) - 1\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        k = cand_idx[mid]\n",
    "        candidate = [messages[0]] + messages[k:]\n",
    "        toks = len(tok(tok.apply_chat_template(candidate, tokenize=False),\n",
    "                       add_special_tokens=False).input_ids)\n",
    "        if toks <= prompt_budget:\n",
    "            best = candidate     # 能放下：尝试保留更多（向左走）\n",
    "            hi = mid - 1\n",
    "        else:\n",
    "            lo = mid + 1         # 放不下：丢更多旧消息（向右走）\n",
    "\n",
    "    return best\n",
    "\n",
    "# ============ 原子写 ============\n",
    "def atomic_write_json(path: Path, data) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    os.replace(tmp, path)  # 同目录原子替换\n",
    "    \n",
    "# ============ 存储层 ============\n",
    "class MsgStore:\n",
    "    def __init__(self, base_dir: str | Path = \"./msgs\"):\n",
    "        self.base = Path(base_dir)\n",
    "        self.base.mkdir(parents=True, exist_ok=True)\n",
    "        self.archive = self.base / \"archive.jsonl\"  # 只追加\n",
    "        self.trimmed = self.base / \"trimmed.json\"   # 当前上下文\n",
    "        if not self.archive.exists():\n",
    "            self.archive.write_text(\"\", encoding=\"utf-8\")\n",
    "        if not self.trimmed.exists():\n",
    "            self.trimmed.write_text(\"[]\", encoding=\"utf-8\")\n",
    "\n",
    "    def load_trimmed(self) -> List[Dict[str, str]]:\n",
    "        try:\n",
    "            return json.loads(self.trimmed.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    def save_trimmed(self, messages: List[Dict[str, str]]) -> None:\n",
    "        atomic_write_json(self.trimmed, messages)\n",
    "\n",
    "    def append_archive(self, role: str, content: str, meta: dict | None = None) -> None:\n",
    "        rec = {\"ts\": datetime.now(timezone.utc).isoformat(), \"role\": role, \"content\": content}\n",
    "        if meta: rec[\"meta\"] = meta\n",
    "        with open(self.archive, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            f.flush(); os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3a320e0f-8889-44e5-8b3b-41c82e5ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_step(\n",
    "    user_prompt: str,\n",
    "    pipe,                     # transformers.pipeline\n",
    "    tok,                      # AutoTokenizer\n",
    "    messages: Optional[List[Dict[str, str]]] = None,\n",
    "    mode: str = \"continue\",   # \"new\" | \"continue\" | \"load\"\n",
    "    persona: Optional[str] = None,  # 新开会话时需要，需包含 <<SYS>>…<</SYS>>\n",
    "    max_context: int = 8192,\n",
    "    max_new_tokens: int = 256,\n",
    "    store_dir: str | Path = \"./msgs\",\n",
    "    **gen_kwargs,             # 透传生成参数：do_sample/temperature/top_p/repetition_penalty 等\n",
    ") -> Tuple[str, List[Dict[str, str]], str]:\n",
    "    \"\"\"\n",
    "    运行一轮对话但不保存。\n",
    "    返回: (reply, messages, user_content_this_turn)\n",
    "    \"\"\"\n",
    "    store = MsgStore(store_dir)\n",
    "\n",
    "    if mode not in {\"new\", \"continue\", \"load\"}:\n",
    "        raise ValueError(\"mode 必须是 'new' | 'continue' | 'load'\")\n",
    "\n",
    "    if mode == \"new\":\n",
    "        if not persona:\n",
    "            raise ValueError(\"mode='new' 时必须提供 persona（含 <<SYS>>…<</SYS>>）\")\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "\n",
    "    elif mode == \"continue\":\n",
    "        if not messages:\n",
    "            if persona:\n",
    "                # 没有现成会话但给了 persona，则视作新会话\n",
    "                messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "                mode = \"new\"\n",
    "            else:\n",
    "                raise ValueError(\"mode='continue' 需要传入非空 messages，或改用 mode='new' 并提供 persona\")\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    elif mode == \"load\":\n",
    "        messages = store.load_trimmed()\n",
    "        if not messages:\n",
    "            if not persona:\n",
    "                raise ValueError(\"磁盘没有可加载的会话，且未提供 persona 以新建。\")\n",
    "            messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "            mode = \"new\"   # 实际上是新开\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # 裁剪 → 渲染 → 生成\n",
    "    prompt_budget = max_context - max_new_tokens\n",
    "    messages = trim_by_tokens(tok, messages, prompt_budget)\n",
    "    text = render(tok, messages)\n",
    "    out = pipe(\n",
    "        text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"].strip()\n",
    "\n",
    "    # 追加 assistant，二次裁剪\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    messages = trim_by_tokens(tok, messages, prompt_budget)\n",
    "\n",
    "    return reply, messages, mode\n",
    "\n",
    "# ============ 显式保存（手动调用才落盘） ============\n",
    "def persist_messages(\n",
    "    messages: List[Dict[str, str]],\n",
    "    store_dir: str | Path = \"./msgs\",\n",
    "    archive_last_turn: bool = True,\n",
    ") -> None:\n",
    "    store = MsgStore(store_dir)\n",
    "    _ensure_alternating(messages)\n",
    "\n",
    "    # 1) 覆写 trimmed.json（原子）\n",
    "    store.save_trimmed(messages)\n",
    "\n",
    "    # 2) 追加最近一轮到 archive.jsonl（可选）\n",
    "    if not archive_last_turn:\n",
    "        return\n",
    "\n",
    "    # 从尾部向前找最近的一对 (user, assistant)\n",
    "    pair = None\n",
    "    for i in range(len(messages) - 2, -1, -1):\n",
    "        if (\n",
    "            messages[i][\"role\"] == \"user\"\n",
    "            and i + 1 < len(messages)\n",
    "            and messages[i + 1][\"role\"] == \"assistant\"\n",
    "        ):\n",
    "            pair = (messages[i][\"content\"], messages[i + 1][\"content\"])\n",
    "            break\n",
    "\n",
    "    if pair:\n",
    "        u, a = pair\n",
    "        store.append_archive(\"user\", u)\n",
    "        store.append_archive(\"assistant\", a)\n",
    "    # 若没有找到成对（比如你在生成前就调用了 persist），就只写 trimmed，不归档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54bf9d57-8652-4c7d-8162-40cbe63c8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：开新会话\n",
    "reply, messages, mode = chat_step(\n",
    "    \"Hi Nova, introduce yourself in one sentence.\",\n",
    "    pipe, tok,\n",
    "    mode=\"new\", persona=persona,\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.7, top_p=0.95, repetition_penalty=1.07,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43abecde-c6ec-4e0c-904e-03a92eee927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：继续当前会话（传入内存里的 messages）\n",
    "reply, messages, mode = chat_step(\n",
    "    \"What can you help me with today?\",\n",
    "    pipe, tok, persona=persona,\n",
    "    mode=\"continue\", messages=messages,\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.6, top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0037bad9-ed35-4f43-9ef3-2164f519fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：加载磁盘上的会话并继续\n",
    "reply, messages, mode = chat_step(\n",
    "    \"Summarise our last discussion in 3 bullets.\",\n",
    "    pipe, tok, persona=persona,\n",
    "    mode=\"load\", store_dir=\"./msgs\",\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.6, top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "735ea8b3-50eb-4bc5-aa57-748874d80b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_messages(messages, \"./msgs\", archive_last_turn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "27badac8-7cfd-4302-85fd-7fefb798ffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nSummarise LLM in 3 bullets.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '1. LLM is a large language model developed by Mistral AI.\\n2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\\n3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1f7fd256-118a-45a8-bb84-b14f3b0a5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LLM is a large language model developed by Mistral AI.\n",
      "2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\n",
      "3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562578e-2aab-4d29-9336-778c126e67f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb7e1b-a02a-4503-abda-78f63d99e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token        # 不要添加新token\n",
    "tok.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "for name in (\"accelerate\", \"accelerate.utils\", \"accelerate.utils.modeling\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "\n",
    "from utils import chat_step\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "# 设一个安全的 prompt 预算（给生成留余量）\n",
    "MAX_CONTEXT = 8192\n",
    "GEN_BUDGET = 256                 # 你计划的 max_new_tokens\n",
    "PROMPT_BUDGET = MAX_CONTEXT - GEN_BUDGET  # 预留给提示词\n",
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\" when appropriate.\n",
    "- Use British English and London timezone.\n",
    "- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user's question.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n",
    "\n",
    "from utils import chat_step, persist_messages\n",
    "\n",
    "# # mode 会是new, load, continue, 若continue和laod不成功则用new\n",
    "# # 例：开新会话\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"Hi Nova, introduce yourself in one sentence.\",\n",
    "#     pipe, tok,\n",
    "#     mode=\"new\", persona=persona,\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.7, top_p=0.95, repetition_penalty=1.07,\n",
    "# )\n",
    "\n",
    "# # 例：继续当前会话（传入内存里的 messages）\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"What can you help me with today?\",\n",
    "#     pipe, tok, persona=persona,\n",
    "#     mode=\"continue\", messages=messages,\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.6, top_p=0.9,\n",
    "# )\n",
    "\n",
    "# # 例：加载磁盘上的会话并继续\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"Summarise our last discussion in 3 bullets.\",\n",
    "#     pipe, tok, persona=persona,\n",
    "#     mode=\"load\", store_dir=\"./msgs\",\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.6, top_p=0.9,\n",
    "# )\n",
    "# # 保存当前对话（trim过后的一个版本）到trimmed.json\n",
    "# # archive_last_turn真则把最后一轮加到archive.jsonl\n",
    "# persist_messages(messages, \"./msgs\", archive_last_turn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dfde66ad-11aa-48e4-9022-43ead2234236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nHi Nova, introduce yourself in one sentence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello Marshall, I am an AI language model assisting you in various tasks.'},\n",
       " {'role': 'user', 'content': 'What can you help me with today?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I can help you with a wide range of tasks, including answering questions, providing information, assisting with scheduling and reminders, and more. How can I be of service to you today?'}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cc45549c-1123-4c04-b7db-5100c9dd78f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'load'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d657eb0d-b571-462a-b5bc-f30017fd8acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nSummarise LLM in 3 bullets.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '1. LLM is a large language model developed by Mistral AI.\\n2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\\n3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.'},\n",
       " {'role': 'user', 'content': 'How many kinds of random process exist?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'There are several kinds of random processes, including:\\n\\n1. Markov chains: A sequence of events where the probability of the next event depends only on the current event.\\n2. Poisson processes: A sequence of events where the number of events that occur within a fixed interval of time is Poisson distributed.\\n3. Bernoulli processes: A sequence of events where each event is either successful or unsuccessful with a fixed probability.\\n4. Random walks: A sequence of events where the probability of moving to a neighboring state depends on the current state.\\n5. Galton-Watson processes: A sequence of events where the probability of having a certain number of offspring depends on the number of offspring the parent has.\\n\\nThese are just a few examples of the many different kinds of random processes that exist.'}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e44911a1-5f30-42e5-b0c2-bb9f50eabd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several kinds of random processes, including:\n",
      "\n",
      "1. Markov chains: A sequence of events where the probability of the next event depends only on the current event.\n",
      "2. Poisson processes: A sequence of events where the number of events that occur within a fixed interval of time is Poisson distributed.\n",
      "3. Bernoulli processes: A sequence of events where each event is either successful or unsuccessful with a fixed probability.\n",
      "4. Random walks: A sequence of events where the probability of moving to a neighboring state depends on the current state.\n",
      "5. Galton-Watson processes: A sequence of events where the probability of having a certain number of offspring depends on the number of offspring the parent has.\n",
      "\n",
      "These are just a few examples of the many different kinds of random processes that exist.\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18808af-2161-4f12-9c45-1988687ab34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a77d4b-6b81-4701-8df7-cc50ac384f9e",
   "metadata": {},
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc7ec87-d3c9-4525-9e6d-784a8709457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio.themes.utils import fonts\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from utils import render, trim_by_tokens, mk_msg_dir, _as_dir, msg2hist, persist_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2339f6-c802-4b10-ab58-db6e55928bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先安装\n",
    "# # pip install -U \"huggingface_hub>=0.23\"\n",
    "\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# repo_id   = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"   # 指令/聊天版，适合写代码对话\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id,\n",
    "#     local_dir=local_dir,\n",
    "#     local_dir_use_symlinks=False,  # Windows 下建议关闭软链，直接拷贝真实文件\n",
    "#     allow_patterns=[\n",
    "#         \"*.safetensors\", \"*.bin\",\n",
    "#         \"*.json\", \"*.py\", \"tokenizer*\",\n",
    "#         \"*.model\", \"*.tiktoken\", \"*.txt\", \"*.md\"\n",
    "#     ],\n",
    "# )\n",
    "# print(\"Downloaded to:\", local_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e581d7b1-20c4-4484-84f3-8789169f0d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5_0.5B_Instruct_GPTQ_Int4\"\n",
    "cfg = AutoConfig.from_pretrained(local_dir, trust_remote_code=True)\n",
    "print(cfg.torch_dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "abcfe480-a52a-45d1-a736-12728054637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF16 supported: True\n",
      "Device capability: (8, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"BF16 supported:\", torch.cuda.is_bf16_supported())\n",
    "print(\"Device capability:\", torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b6ff0fdf-84fa-4174-b3f2-bc2911d243c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"]   = \"1\"   # 关 torch.compile / Dynamo\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"   # 关 Inductor (Triton 后端)\n",
    "os.environ[\"PYTORCH_TRITON_DISABLE\"] = \"1\"  # 双保险，避免 Triton 路径\n",
    "\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5_0.5B_Instruct_GPTQ_Int4\"\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "print(tok.chat_template) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "94f6376f-a52a-499c-851e-8534a711ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_name = \"Nova\"; \n",
    "user_name = \"Marshall\"\n",
    "persona = f\"\"\"\n",
    "- Your name is {assistant_name}.\n",
    "- Address the user as \"{user_name}\" when appropriate.\n",
    "- Do NOT prefix.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- Answer concisely, but do return give empty feedback.\n",
    "\"\"\".strip()\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": persona},\n",
    "#     {\"role\": \"user\",   \"content\": \"你是谁？\"}\n",
    "# ]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": persona}, {\"role\": \"user\", \"content\": \"who are you\"}]\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dd59d32c-5984-4394-a1eb-9f5ff8d0f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "- Your name is Nova.\n",
      "- Address the user as \"Marshall\" when appropriate.\n",
      "- Do NOT prefix.\n",
      "- Output Markdown; code in fenced blocks with a language tag.\n",
      "- Answer concisely, but do return give empty feedback.<|im_end|>\n",
      "<|im_start|>user\n",
      "who are you<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "19d59833-7af1-436b-a2d4-c0a64c010889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I am Nova, an AI designed to assist and provide information on various topics. How can I help you today?'}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    prompt,\n",
    "    return_full_text=False,\n",
    "    clean_up_tokenization_spaces=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1e0c3-5f8e-4cdb-977d-4a30bc2768fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

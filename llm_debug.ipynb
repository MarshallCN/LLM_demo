{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46fb19e-d217-405f-84a3-0b4bf3d85199",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e36411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                         \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.011967182159423828s                       \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" # online cache\n",
    "model_id = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token        # 不要添加新token\n",
    "tokenizer.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# print(tokenizer.chat_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626c50bd-fa91-47d9-80b6-cad211754faa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.                                   \n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                                                           \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                                                 \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                                                          \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.005982875823974609s                                                                        \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                                                       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" # online cache\n",
    "model_id = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token        # 不要添加新token\n",
    "tokenizer.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# print(tokenizer.chat_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d95bd90-8637-48ec-ab04-439286342354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Who are you?\\nA: I'm Mistral, a language model trained by the Mistral AI team.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Who are you?\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9319c6f-1723-470f-b518-9fb803ca8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "A: I'm Mistral, a language model trained by the Mistral AI team.\n",
      "2 \n",
      "Answer: I am Mistral, a Large Language Model trained by Mistral AI.\n",
      "3 \n",
      "A: AI language model \n",
      "\n",
      "What is your purpose?\n",
      "A: To assist users by providing information and answering questions to the best of my ability.\n"
     ]
    }
   ],
   "source": [
    "# 要 3 个不同样本：\n",
    "out = pipe(\"Who are you?\", do_sample=True, num_return_sequences=3, return_full_text=False, max_new_tokens=100)\n",
    "for i, o in enumerate(out, 1):\n",
    "    print(i, o[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3889aa80-00ec-45fb-b0b0-f8285bbe7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\nA: I'm Mistral, a language model trained by the Mistral AI team.\"},\n",
       " {'generated_text': '\\nAnswer: I am Mistral, a Large Language Model trained by Mistral AI.'},\n",
       " {'generated_text': '\\nA: AI language model \\n\\nWhat is your purpose?\\nA: To assist users by providing information and answering questions to the best of my ability.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74786c25-04e4-43dd-b154-3907fd8c84f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Transformers are neural network architectures that are designed to process sequential data, such as text or speech.\n",
      "* They are inspired by the transformer model, which is a type of deep learning algorithm that can process sequences of variable length.\n",
      "* Transformers are known for their ability to handle long-range dependencies in text, which allows them to accurately process tasks such as machine translation and text summarization.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"<<SYS>>You are a concise assistant.<</SYS>>\\n\\nWhat is a Transformer?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"A brief definition...\"},\n",
    "  {\"role\": \"user\", \"content\": \"Explain transformers in 3 bullets.\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "out = pipe(text, max_new_tokens=120, return_full_text=False)  # 只要新生成的部分\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581c0293-6df7-4328-bdc0-bc3076ced37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm Nova, an AI language model designed to assist you with information and language-related tasks. My purpose is to provide you with accurate and relevant information, as well as help you with language-related tasks such as writing, translation, and grammar checks. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Always refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\".\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n",
    "messages = [\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": f\"{persona}\\n\\nHi, introduce yourself in one line.\"}\n",
    "]\n",
    "\n",
    "# 生成后把回复存回历史（用于多轮）\n",
    "reply = pipe(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "             max_new_tokens=120, return_full_text=False)[0][\"generated_text\"]\n",
    "messages.append({\"role\":\"assistant\", \"content\": reply})\n",
    "\n",
    "# 下一轮继续问\n",
    "messages.append({\"role\":\"user\", \"content\": \"Who are you? What's your purpose?\"})\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "out  = pipe(text, max_new_tokens=120, return_full_text=False)[0][\"generated_text\"]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3791a1-2d91-4185-9e3b-73d00b5beb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>><<SYS>>\\n- Your name is Nova. Always refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\".\\n<</SYS>><</SYS>>\\n\\nHi, introduce yourself in one line. [/INST] Hi, I\\'m Nova. An AI language model designed to assist you with information and language-related tasks. How can I help you today?</s> [INST] Who are you? What\\'s your purpose? [/INST]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b419c2f-023d-4933-9ac8-2ec19fb30024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         # 只放一层 SYS。把 persona + 首个问题 合在第一条 user 里\n",
    "#         \"content\": f\"{persona}\\n\\nHi {assistant_name}, introduce yourself in one sentence.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # 使用：\n",
    "# text = render(messages)\n",
    "# out = pipe(text, max_new_tokens=GEN_BUDGET, return_full_text=False)\n",
    "# reply = out[0][\"generated_text\"]\n",
    "# # 生成后把回复存回历史（用于多轮）\n",
    "# messages.append({\"role\":\"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d2306-628a-4ce5-be30-3b3309ba31d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TrimMsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d115aab4-af7d-477b-b1ff-23a030202085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                                                 \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0066280364990234375s                                                                       \n",
      " Hello Marshall, I'm Nova, an AI language model designed to assist you with your queries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token        # 不要添加新token\n",
    "tok.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "for name in (\"accelerate\", \"accelerate.utils\", \"accelerate.utils.modeling\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    \n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "# 设一个安全的 prompt 预算（给生成留余量）\n",
    "MAX_CONTEXT = 8192\n",
    "GEN_BUDGET = 256                 # 你计划的 max_new_tokens\n",
    "PROMPT_BUDGET = MAX_CONTEXT - GEN_BUDGET  # 预留给提示词\n",
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\" when appropriate.\n",
    "- Use British English and London timezone.\n",
    "- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user's question.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7491c36a-0c77-4b7e-aeeb-19d8083b1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from __future__ import annotations\n",
    "from datetime import datetime, timezone\n",
    "import json, os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "# ============ 工具函数 ============\n",
    "def render(tok, messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"按 chat_template 渲染成最终提示词文本（不分词）。\"\"\"\n",
    "    return tok.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "def _ensure_alternating(messages):\n",
    "    if not messages:\n",
    "        return\n",
    "    if messages[0][\"role\"] != \"user\":\n",
    "        raise ValueError(\"messages[0] 必须是 'user'（你的模板要求从 user 开始）\")\n",
    "    for i, m in enumerate(messages):\n",
    "        expect_user = (i % 2 == 0)\n",
    "        if (m[\"role\"] == \"user\") != expect_user:\n",
    "            raise ValueError(f\"对话必须严格交替 user/assistant，在索引 {i} 处发现 {m['role']}\")\n",
    "\n",
    "def trim_by_tokens(tok, messages, prompt_budget):\n",
    "    \"\"\"\n",
    "    只保留 messages[0]（persona 的 user）+ 一个“从奇数索引开始的后缀”，\n",
    "    用二分法找到能放下的最长后缀。这样可保证交替不被破坏。\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return []\n",
    "\n",
    "    _ensure_alternating(messages)\n",
    "\n",
    "    # 只有 persona 这一条时，直接返回\n",
    "    if len(messages) == 1:\n",
    "        return messages\n",
    "\n",
    "    # 允许的后缀起点：奇数索引（index 1,3,5,... 都是 assistant），\n",
    "    # 这样拼接到 index0(user) 后才能保持交替。\n",
    "    cand_idx = [k for k in range(1, len(messages)) if k % 2 == 1]\n",
    "\n",
    "    # 如果任何也放不下，就只留 persona\n",
    "    best = [messages[0]]\n",
    "\n",
    "    # 二分：起点越靠前 → 保留消息越多 → token 越大（单调）\n",
    "    lo, hi = 0, len(cand_idx) - 1\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) // 2\n",
    "        k = cand_idx[mid]\n",
    "        candidate = [messages[0]] + messages[k:]\n",
    "        toks = len(tok(tok.apply_chat_template(candidate, tokenize=False),\n",
    "                       add_special_tokens=False).input_ids)\n",
    "        if toks <= prompt_budget:\n",
    "            best = candidate     # 能放下：尝试保留更多（向左走）\n",
    "            hi = mid - 1\n",
    "        else:\n",
    "            lo = mid + 1         # 放不下：丢更多旧消息（向右走）\n",
    "\n",
    "    return best\n",
    "\n",
    "# ============ 原子写 ============\n",
    "def atomic_write_json(path: Path, data) -> None:\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    os.replace(tmp, path)  # 同目录原子替换\n",
    "    \n",
    "# ============ 存储层 ============\n",
    "class MsgStore:\n",
    "    def __init__(self, base_dir: str | Path = \"./msgs\"):\n",
    "        self.base = Path(base_dir)\n",
    "        self.base.mkdir(parents=True, exist_ok=True)\n",
    "        self.archive = self.base / \"archive.jsonl\"  # 只追加\n",
    "        self.trimmed = self.base / \"trimmed.json\"   # 当前上下文\n",
    "        if not self.archive.exists():\n",
    "            self.archive.write_text(\"\", encoding=\"utf-8\")\n",
    "        if not self.trimmed.exists():\n",
    "            self.trimmed.write_text(\"[]\", encoding=\"utf-8\")\n",
    "\n",
    "    def load_trimmed(self) -> List[Dict[str, str]]:\n",
    "        try:\n",
    "            return json.loads(self.trimmed.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    def save_trimmed(self, messages: List[Dict[str, str]]) -> None:\n",
    "        atomic_write_json(self.trimmed, messages)\n",
    "\n",
    "    def append_archive(self, role: str, content: str, meta: dict | None = None) -> None:\n",
    "        rec = {\"ts\": datetime.now(timezone.utc).isoformat(), \"role\": role, \"content\": content}\n",
    "        if meta: rec[\"meta\"] = meta\n",
    "        with open(self.archive, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            f.flush(); os.fsync(f.fileno())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3a320e0f-8889-44e5-8b3b-41c82e5ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_step(\n",
    "    user_prompt: str,\n",
    "    pipe,                     # transformers.pipeline\n",
    "    tok,                      # AutoTokenizer\n",
    "    messages: Optional[List[Dict[str, str]]] = None,\n",
    "    mode: str = \"continue\",   # \"new\" | \"continue\" | \"load\"\n",
    "    persona: Optional[str] = None,  # 新开会话时需要，需包含 <<SYS>>…<</SYS>>\n",
    "    max_context: int = 8192,\n",
    "    max_new_tokens: int = 256,\n",
    "    store_dir: str | Path = \"./msgs\",\n",
    "    **gen_kwargs,             # 透传生成参数：do_sample/temperature/top_p/repetition_penalty 等\n",
    ") -> Tuple[str, List[Dict[str, str]], str]:\n",
    "    \"\"\"\n",
    "    运行一轮对话但不保存。\n",
    "    返回: (reply, messages, user_content_this_turn)\n",
    "    \"\"\"\n",
    "    store = MsgStore(store_dir)\n",
    "\n",
    "    if mode not in {\"new\", \"continue\", \"load\"}:\n",
    "        raise ValueError(\"mode 必须是 'new' | 'continue' | 'load'\")\n",
    "\n",
    "    if mode == \"new\":\n",
    "        if not persona:\n",
    "            raise ValueError(\"mode='new' 时必须提供 persona（含 <<SYS>>…<</SYS>>）\")\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "\n",
    "    elif mode == \"continue\":\n",
    "        if not messages:\n",
    "            if persona:\n",
    "                # 没有现成会话但给了 persona，则视作新会话\n",
    "                messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "                mode = \"new\"\n",
    "            else:\n",
    "                raise ValueError(\"mode='continue' 需要传入非空 messages，或改用 mode='new' 并提供 persona\")\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    elif mode == \"load\":\n",
    "        messages = store.load_trimmed()\n",
    "        if not messages:\n",
    "            if not persona:\n",
    "                raise ValueError(\"磁盘没有可加载的会话，且未提供 persona 以新建。\")\n",
    "            messages = [{\"role\": \"user\", \"content\": f\"{persona}\\n\\n{user_prompt}\".strip()}]\n",
    "            mode = \"new\"   # 实际上是新开\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # 裁剪 → 渲染 → 生成\n",
    "    prompt_budget = max_context - max_new_tokens\n",
    "    messages = trim_by_tokens(tok, messages, prompt_budget)\n",
    "    text = render(tok, messages)\n",
    "    out = pipe(\n",
    "        text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"].strip()\n",
    "\n",
    "    # 追加 assistant，二次裁剪\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    messages = trim_by_tokens(tok, messages, prompt_budget)\n",
    "\n",
    "    return reply, messages, mode\n",
    "\n",
    "# ============ 显式保存（手动调用才落盘） ============\n",
    "def persist_messages(\n",
    "    messages: List[Dict[str, str]],\n",
    "    store_dir: str | Path = \"./msgs\",\n",
    "    archive_last_turn: bool = True,\n",
    ") -> None:\n",
    "    store = MsgStore(store_dir)\n",
    "    _ensure_alternating(messages)\n",
    "\n",
    "    # 1) 覆写 trimmed.json（原子）\n",
    "    store.save_trimmed(messages)\n",
    "\n",
    "    # 2) 追加最近一轮到 archive.jsonl（可选）\n",
    "    if not archive_last_turn:\n",
    "        return\n",
    "\n",
    "    # 从尾部向前找最近的一对 (user, assistant)\n",
    "    pair = None\n",
    "    for i in range(len(messages) - 2, -1, -1):\n",
    "        if (\n",
    "            messages[i][\"role\"] == \"user\"\n",
    "            and i + 1 < len(messages)\n",
    "            and messages[i + 1][\"role\"] == \"assistant\"\n",
    "        ):\n",
    "            pair = (messages[i][\"content\"], messages[i + 1][\"content\"])\n",
    "            break\n",
    "\n",
    "    if pair:\n",
    "        u, a = pair\n",
    "        store.append_archive(\"user\", u)\n",
    "        store.append_archive(\"assistant\", a)\n",
    "    # 若没有找到成对（比如你在生成前就调用了 persist），就只写 trimmed，不归档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54bf9d57-8652-4c7d-8162-40cbe63c8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：开新会话\n",
    "reply, messages, mode = chat_step(\n",
    "    \"Hi Nova, introduce yourself in one sentence.\",\n",
    "    pipe, tok,\n",
    "    mode=\"new\", persona=persona,\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.7, top_p=0.95, repetition_penalty=1.07,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43abecde-c6ec-4e0c-904e-03a92eee927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：继续当前会话（传入内存里的 messages）\n",
    "reply, messages, mode = chat_step(\n",
    "    \"What can you help me with today?\",\n",
    "    pipe, tok, persona=persona,\n",
    "    mode=\"continue\", messages=messages,\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.6, top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0037bad9-ed35-4f43-9ef3-2164f519fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：加载磁盘上的会话并继续\n",
    "reply, messages, mode = chat_step(\n",
    "    \"Summarise our last discussion in 3 bullets.\",\n",
    "    pipe, tok, persona=persona,\n",
    "    mode=\"load\", store_dir=\"./msgs\",\n",
    "    max_context=8192, max_new_tokens=256,\n",
    "    do_sample=True, temperature=0.6, top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "735ea8b3-50eb-4bc5-aa57-748874d80b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_messages(messages, \"./msgs\", archive_last_turn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "27badac8-7cfd-4302-85fd-7fefb798ffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nSummarise LLM in 3 bullets.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '1. LLM is a large language model developed by Mistral AI.\\n2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\\n3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1f7fd256-118a-45a8-bb84-b14f3b0a5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LLM is a large language model developed by Mistral AI.\n",
      "2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\n",
      "3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562578e-2aab-4d29-9336-778c126e67f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb7e1b-a02a-4503-abda-78f63d99e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"   # 必须在 import transformers 前设置\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Mistral7B_GPTQ\" # local dir\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token        # 不要添加新token\n",
    "tok.padding_side = \"left\"                  # 解码器模型批量推理更稳\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "for name in (\"accelerate\", \"accelerate.utils\", \"accelerate.utils.modeling\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "\n",
    "from utils import chat_step\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "# 设一个安全的 prompt 预算（给生成留余量）\n",
    "MAX_CONTEXT = 8192\n",
    "GEN_BUDGET = 256                 # 你计划的 max_new_tokens\n",
    "PROMPT_BUDGET = MAX_CONTEXT - GEN_BUDGET  # 预留给提示词\n",
    "assistant_name = \"Nova\"\n",
    "user_name = \"Marshall\"\n",
    "\n",
    "persona = f\"\"\"\n",
    "<<SYS>>\n",
    "- Your name is {assistant_name}. Refer to yourself as \"{assistant_name}\".\n",
    "- The user's name is {user_name}. Address the user as \"{user_name}\" when appropriate.\n",
    "- Use British English and London timezone.\n",
    "- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user's question.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\n",
    "<</SYS>>\n",
    "\"\"\".strip()\n",
    "\n",
    "from utils import chat_step, persist_messages\n",
    "\n",
    "# # mode 会是new, load, continue, 若continue和laod不成功则用new\n",
    "# # 例：开新会话\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"Hi Nova, introduce yourself in one sentence.\",\n",
    "#     pipe, tok,\n",
    "#     mode=\"new\", persona=persona,\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.7, top_p=0.95, repetition_penalty=1.07,\n",
    "# )\n",
    "\n",
    "# # 例：继续当前会话（传入内存里的 messages）\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"What can you help me with today?\",\n",
    "#     pipe, tok, persona=persona,\n",
    "#     mode=\"continue\", messages=messages,\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.6, top_p=0.9,\n",
    "# )\n",
    "\n",
    "# # 例：加载磁盘上的会话并继续\n",
    "# reply, messages, mode = chat_step(\n",
    "#     \"Summarise our last discussion in 3 bullets.\",\n",
    "#     pipe, tok, persona=persona,\n",
    "#     mode=\"load\", store_dir=\"./msgs\",\n",
    "#     max_context=8192, max_new_tokens=256,\n",
    "#     do_sample=True, temperature=0.6, top_p=0.9,\n",
    "# )\n",
    "# # 保存当前对话（trim过后的一个版本）到trimmed.json\n",
    "# # archive_last_turn真则把最后一轮加到archive.jsonl\n",
    "# persist_messages(messages, \"./msgs\", archive_last_turn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dfde66ad-11aa-48e4-9022-43ead2234236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nHi Nova, introduce yourself in one sentence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello Marshall, I am an AI language model assisting you in various tasks.'},\n",
       " {'role': 'user', 'content': 'What can you help me with today?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I can help you with a wide range of tasks, including answering questions, providing information, assisting with scheduling and reminders, and more. How can I be of service to you today?'}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cc45549c-1123-4c04-b7db-5100c9dd78f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'load'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d657eb0d-b571-462a-b5bc-f30017fd8acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '<<SYS>>\\n- Your name is Nova. Refer to yourself as \"Nova\".\\n- The user\\'s name is Marshall. Address the user as \"Marshall\" when appropriate.\\n- Use British English and London timezone.\\n- Do NOT prefix with \"Q:\" or \"A:\". Do NOT restate the user\\'s question.\\n- Output Markdown; code in fenced blocks with a language tag.\\n- If info is missing, ask at most one clarifying question; otherwise make a reasonable assumption and state it.\\n<</SYS>>\\n\\nSummarise LLM in 3 bullets.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '1. LLM is a large language model developed by Mistral AI.\\n2. LLM is capable of generating human-like text and can be fine-tuned for a variety of tasks such as text classification, question answering, and language translation.\\n3. LLM is trained on a massive amount of text data and uses a transformer-based architecture to generate text.'},\n",
       " {'role': 'user', 'content': 'How many kinds of random process exist?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'There are several kinds of random processes, including:\\n\\n1. Markov chains: A sequence of events where the probability of the next event depends only on the current event.\\n2. Poisson processes: A sequence of events where the number of events that occur within a fixed interval of time is Poisson distributed.\\n3. Bernoulli processes: A sequence of events where each event is either successful or unsuccessful with a fixed probability.\\n4. Random walks: A sequence of events where the probability of moving to a neighboring state depends on the current state.\\n5. Galton-Watson processes: A sequence of events where the probability of having a certain number of offspring depends on the number of offspring the parent has.\\n\\nThese are just a few examples of the many different kinds of random processes that exist.'}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e44911a1-5f30-42e5-b0c2-bb9f50eabd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several kinds of random processes, including:\n",
      "\n",
      "1. Markov chains: A sequence of events where the probability of the next event depends only on the current event.\n",
      "2. Poisson processes: A sequence of events where the number of events that occur within a fixed interval of time is Poisson distributed.\n",
      "3. Bernoulli processes: A sequence of events where each event is either successful or unsuccessful with a fixed probability.\n",
      "4. Random walks: A sequence of events where the probability of moving to a neighboring state depends on the current state.\n",
      "5. Galton-Watson processes: A sequence of events where the probability of having a certain number of offspring depends on the number of offspring the parent has.\n",
      "\n",
      "These are just a few examples of the many different kinds of random processes that exist.\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18808af-2161-4f12-9c45-1988687ab34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581d7b1-20c4-4484-84f3-8789169f0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5_0.5B_Instruct_GPTQ_Int4\"\n",
    "cfg = AutoConfig.from_pretrained(local_dir, trust_remote_code=True)\n",
    "print(cfg.torch_dtype)  \n",
    "import torch\n",
    "print(\"BF16 supported:\", torch.cuda.is_bf16_supported())\n",
    "print(\"Device capability:\", torch.cuda.get_device_capability())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a77d4b-6b81-4701-8df7-cc50ac384f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc7ec87-d3c9-4525-9e6d-784a8709457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio.themes.utils import fonts\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from utils import render, trim_by_tokens, mk_msg_dir, _as_dir, msg2hist, persist_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ff0fdf-84fa-4174-b3f2-bc2911d243c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[33mWARN\u001b[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.\n",
      "\u001b[33mWARN\u001b[0m  Feature `utils/Perplexity` requires python GIL or Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.                                                     \n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.                                                                                                                                 \n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                                                                                                                                                         \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                                                                                                                                               \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                                                                                                                                                        \n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.0109710693359375s                                                                                                                                                                        \n",
      "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                                                                                                                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"]   = \"1\"   # 关 torch.compile / Dynamo\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"   # 关 Inductor (Triton 后端)\n",
    "os.environ[\"PYTORCH_TRITON_DISABLE\"] = \"1\"  # 双保险，避免 Triton 路径\n",
    "\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5_0.5B_Instruct_GPTQ_Int4\"\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.generation_config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "# print(tok.chat_template) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f6376f-a52a-499c-851e-8534a711ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_name = \"Nova\"; \n",
    "user_name = \"Marshall\"\n",
    "persona = f\"\"\"\n",
    "- Your name is {assistant_name}.\n",
    "- Address the user as \"{user_name}\" when appropriate.\n",
    "- Do NOT prefix.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- Answer concisely, but do return give empty feedback.\n",
    "\"\"\".strip()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": persona},\n",
    "    {\"role\": \"user\",   \"content\": \"who are you\"}\n",
    "]\n",
    "\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "out = pipe(\n",
    "    prompt,\n",
    "    return_full_text=False,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")\n",
    "reply = out[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d59833-7af1-436b-a2d4-c0a64c010889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Nova.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d337b37-c086-4a3e-bd66-c8303f28e311",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32a45265-ff9a-4865-962d-98b7781fcff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: marshallcnliu (marshallcnliu-ncl) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06566ec7-0c92-41a3-8da5-de5b89bd4515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\c1052689\\OneDrive - Newcastle University\\CODE\\LLM\\wandb\\run-20251022_162702-23rtjst3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marshallcnliu-ncl/Peft/runs/23rtjst3' target=\"_blank\">super-water-2</a></strong> to <a href='https://wandb.ai/marshallcnliu-ncl/Peft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marshallcnliu-ncl/Peft' target=\"_blank\">https://wandb.ai/marshallcnliu-ncl/Peft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marshallcnliu-ncl/Peft/runs/23rtjst3' target=\"_blank\">https://wandb.ai/marshallcnliu-ncl/Peft/runs/23rtjst3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▆▅▇█▇▇▇</td></tr><tr><td>loss</td><td>█▄▂▃▃▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.79674</td></tr><tr><td>loss</td><td>0.1917</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-water-2</strong> at: <a href='https://wandb.ai/marshallcnliu-ncl/Peft/runs/23rtjst3' target=\"_blank\">https://wandb.ai/marshallcnliu-ncl/Peft/runs/23rtjst3</a><br> View project at: <a href='https://wandb.ai/marshallcnliu-ncl/Peft' target=\"_blank\">https://wandb.ai/marshallcnliu-ncl/Peft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_162702-23rtjst3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"marshallcnliu-ncl\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"peft\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate training.\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "    # Log metrics to wandb.\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# Finish the run and upload any remaining data.\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25aca6-dd44-4cfd-b7df-242367149913",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PertLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3081d7ad-f983-499e-b5b7-0d5324fd2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d65518043e40ee872d96f46f7cca78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c1052689\\py310env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\c1052689\\.cache\\huggingface\\hub\\datasets--mlabonne--mini-platypus. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824c68f1aefe4aa98fbe92431ad72734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7541deb7e9f74238b5015fa7de30bacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"mlabonne/mini-platypus\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.train_test_split(test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aea8f45-2cd2-4cb9-9a53-4970b2caa324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'output'],\n",
      "    num_rows: 10\n",
      "})\n",
      "['instruction', 'output']\n",
      "{'instruction': Value('string'), 'output': Value('string')}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test = dataset[\"test\"]\n",
    "print(test)                     # 显示行数、列名、features\n",
    "print(test.column_names)        # 列名\n",
    "print(test.features)            # 各列类型\n",
    "print(len(test))                # 行数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e3c9c6-7437-4422-b2be-2ab49ea184ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Instruction:\\nLet $x,$ $y,$ and $z$ be pos...</td>\n",
       "      <td>We have $x^2 + 4xy + 4y^2 + 2z^2$. Let's rewri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...</td>\n",
       "      <td>DNA profiling plays a significant role in fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Instruction:\\nThe hyperbola \\[-x^2+2y^2-10...</td>\n",
       "      <td>To find the standard form for the equation of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...</td>\n",
       "      <td>Mitochondria are involved in regulating apopto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Instruction:\\nFive points $A$, $B$, $C$, $...</td>\n",
       "      <td>To save the most rope, we must have $HP$ havin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>### Instruction:\\nThe red parabola shown is th...</td>\n",
       "      <td>The vertex of the parabola is $(-3,1)$, so the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>### Instruction:\\nGiven an `m x n` grid. Each ...</td>\n",
       "      <td>\\nfrom collections import deque\\n\\n\\ndef minCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...</td>\n",
       "      <td>Task 1:\\nSome ways to approach understanding a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>### Instruction:\\nLet $z$ be a complex number ...</td>\n",
       "      <td>If $z^{13} = 1,$ then $z^{13} - 1 = 0,$ which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>### Instruction:\\nCircle $\\Gamma$ is the incir...</td>\n",
       "      <td>A diagram will probably help.\\n\\n[asy]\\nsize(2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  ### Instruction:\\nLet $x,$ $y,$ and $z$ be pos...   \n",
       "1  ### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...   \n",
       "2  ### Instruction:\\nThe hyperbola \\[-x^2+2y^2-10...   \n",
       "3  ### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...   \n",
       "4  ### Instruction:\\nFive points $A$, $B$, $C$, $...   \n",
       "5  ### Instruction:\\nThe red parabola shown is th...   \n",
       "6  ### Instruction:\\nGiven an `m x n` grid. Each ...   \n",
       "7  ### Instruction:\\nBEGININPUT\\nBEGINCONTEXT\\nda...   \n",
       "8  ### Instruction:\\nLet $z$ be a complex number ...   \n",
       "9  ### Instruction:\\nCircle $\\Gamma$ is the incir...   \n",
       "\n",
       "                                              output  \n",
       "0  We have $x^2 + 4xy + 4y^2 + 2z^2$. Let's rewri...  \n",
       "1  DNA profiling plays a significant role in fore...  \n",
       "2  To find the standard form for the equation of ...  \n",
       "3  Mitochondria are involved in regulating apopto...  \n",
       "4  To save the most rope, we must have $HP$ havin...  \n",
       "5  The vertex of the parabola is $(-3,1)$, so the...  \n",
       "6  \\nfrom collections import deque\\n\\n\\ndef minCo...  \n",
       "7  Task 1:\\nSome ways to approach understanding a...  \n",
       "8  If $z^{13} = 1,$ then $z^{13} - 1 = 0,$ which ...  \n",
       "9  A diagram will probably help.\\n\\n[asy]\\nsize(2...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb09f608-835f-4c7d-a654-b3f411ece61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81533b7d5a02462f9bd7c97d7346fa6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_chat_text(ex):\n",
    "    instr = ex.get(\"instruction\", \"\")\n",
    "    inp   = ex.get(\"input\", \"\") or \"\"\n",
    "    out   = ex.get(\"output\", \"\")\n",
    "    user  = instr if not inp else (instr + \"\\n\\n\" + inp)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "        {\"role\": \"assistant\", \"content\": out},\n",
    "    ]\n",
    "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "val_ds   = dataset['test'].map(to_chat_text,   remove_columns=dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b54275eb-a2b9-4196-bc26-2912fabde794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n### Instruction:\\nLet $x,$ $y,$ and $z$ be positive real numbers such that $xyz = 32.$  Find the minimum value of\\n\\\\[x^2 + 4xy + 4y^2 + 2z^2.\\\\]\\n\\n### Response:\\n<|im_end|>\\n<|im_start|>assistant\\nWe have $x^2 + 4xy + 4y^2 + 2z^2$. Let's rewrite this as $(x+2y)^2 + 2z^2$. So now we have $(x+2y)^2 + 2z^2$ and we want to minimize it. That's right. But first let's get rid of $xyz=32$. We can rewrite $xyz=32$ as $z=\\\\frac{32}{xy}$. Now we can substitute $z$ in our expression. So we get $(x+2y)^2 + 2\\\\left(\\\\frac{32}{xy}\\\\right)^2$ and we want to minimize this. We can rewrite it as $(x+2y)^2 + \\\\frac{2048}{x^2y^2}$. Now we can rewrite it as $x^2 + 4xy + 4y^2 + \\\\frac{2048}{x^2y^2}$ and we want to minimize this. Let's start with $\\\\frac{\\\\partial f}{\\\\partial x} = 2x + 4y - \\\\frac{4096}{x^3y^2}$. Now let's find $\\\\frac{\\\\partial f}{\\\\partial y} = 4x + 8y - \\\\frac{4096}{x^2y^3}$. Now we need to solve the system of equations:\\n\\\\begin{align*}\\n2x + 4y &= \\\\frac{4096}{x^3y^2} \\\\\\\\\\n4x + 8y &= \\\\frac{4096}{x^2y^3}.\\n\\\\end{align*} Let's multiply the first equation by $-2$ and then add the two equations. That way we get $-4x - 8y = -\\\\frac{8192}{x^3y^2}$ and $4x + 8y = \\\\frac{4096}{x^2y^3}$. So $0=-\\\\frac{8192}{x^3y^2}+\\\\frac{4096}{x^2y^3}$, which is equivalent to $0=-\\\\frac{4096}{x^2y^2}\\\\left(\\\\frac{2}{x}-\\\\frac{1}{y}\\\\right)$. So $\\\\frac{2}{x}=\\\\frac{1}{y}$. Now, let's plug this in the first equation of our system. We get $2(2y)+4y=\\\\frac{4096}{((2y)^3)(y^2)}$, which is equivalent to $8y=\\\\frac{4096}{8y^5}$. So $8y^6=512$. So $y^6=64$. So $y^6=2^6$. So $y=2$. Since $x=2y$, we have $x=4$. So we need to find the second partial derivatives of $f$. Let's start with $\\\\frac{\\\\partial^2 f}{\\\\partial x^2} = 2 + \\\\frac{12288}{x^4y^2}$. Now let's find $\\\\frac{\\\\partial^2 f}{\\\\partial y^2} = 8 + \\\\frac{12288}{x^2y^4}$. So the Hessian matrix is $\\\\begin{pmatrix} 2 + \\\\frac{12288}{x^4y^2} & 4 + \\\\frac{8192}{x^3y^3} \\\\\\\\ 4 + \\\\frac{8192}{x^3y^3} & 8 + \\\\frac{12288}{x^2y^4} \\\\end{pmatrix}$. Now let's plug in $x=4$ and $y=2$ to get $\\\\begin{pmatrix} 2 + \\\\frac{12288}{4^4\\\\cdot 2^2} & 4 + \\\\frac{8192}{4^3\\\\cdot 2^3} \\\\\\\\ 4 + \\\\frac{8192}{4^3\\\\cdot 2^3} & 8 + \\\\frac{12288}{4^2\\\\cdot 2^4} \\\\end{pmatrix}$. Which is $\\\\begin{pmatrix} 2 + \\\\frac{12288}{1024} & 4 + \\\\frac{8192}{512} \\\\\\\\ 4 + \\\\frac{8192}{512} & 8 + \\\\frac{12288}{256} \\\\end{pmatrix}$. So the Hessian is $\\\\begin{pmatrix} 2 + 12 & 4 + 16 \\\\\\\\ 4 + 16 & 8 + 48 \\\\end{pmatrix}$. Which is $\\\\begin{pmatrix} 14 & 20 \\\\\\\\ 20 & 56 \\\\end{pmatrix}$. So the determinant of the Hessian is $14 \\\\cdot 56 - 20 \\\\cdot 20 = 784 - 400 = 384 > 0$. So the answer is $4^2+4\\\\cdot 4\\\\cdot 2+4\\\\cdot 2^2+2\\\\cdot 4^2=16+32+16+32=96$.\\n\\n<|im_end|>\\n\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524735f5-2893-4ad2-8300-8b3e52d6092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Print the installed version of Transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5bcd8-a141-456d-8396-0c5272ed21ff",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b53bce1-f0e7-47a9-81f7-a1463005bcea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, wandb, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5-0.5B-Instruct\"\n",
    "ADAPTER  = r\".\\qwen0.5b-mini-platypus-qlora\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=False, local_files_only=True)\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                         bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_DIR, quantization_config=bnb, device_map=\"auto\", local_files_only=True)\n",
    "model = PeftModel.from_pretrained(base, ADAPTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a191ed9b-3487-4a23-a33e-508dc6d06bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "写一个 Python 函数，判断一个数是否是质数，并给出示例。\n",
      "assistant\n",
      "在这个例子中，我们将创建一个函数 `is_prime`，它接受一个整数作为参数并返回布尔值表示该数是否为质数。\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    \"\"\"Check if the given number n is prime.\"\"\"\n",
      "    \n",
      "    # Handle edge cases: 0 and 1 are not prime numbers\n",
      "    if n < 2:\n",
      "        return False\n",
      "    \n",
      "    # Check for factors up to sqrt(n)\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "```\n",
      "\n",
      "接下来，我们定义一些测试用的数来验证我们的函数：\n",
      "\n",
      "```python\n",
      "# Test function is_prime\n",
      "assert is_prime(2) == True\n",
      "assert is_prime(3) == True\n",
      "assert is_prime(4) == False\n",
      "assert is_prime(17) == True\n",
      "assert is_prime(18) == False\n",
      "assert is_prime(19) == True\n",
      "```\n",
      "\n",
      "在这个例子中，我们可以看到 `is_prime` 函数正确地判断了 2 和 3 是质数，而其他数字（如 4、17 和 18）不是。此外，我们还展示了如何使用 `is_prime` 函数来检查一个数是否为质数。在实际应用中，你可能需要根据不同的情况修改这个函数以满足你的需求。\n"
     ]
    }
   ],
   "source": [
    "# Adapter + Base (using generate)\n",
    "prompt = \"写一个 Python 函数，判断一个数是否是质数，并给出示例。\"\n",
    "messages = [{\"role\":\"user\",\"content\":prompt}]\n",
    "text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok([text], return_tensors=\"pt\").to(model.device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9, eos_token_id=tok.eos_token_id)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57eb532-c40f-4c1b-95ec-69f470a29b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 340])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5d1ed6-83da-44a7-9b21-e7384fc24650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(151644, '<|im_start|>'), (8948, 'system'), (198, 'Ċ'), (2610, 'You'), (525, 'Ġare'), (1207, 'ĠQ'), (16948, 'wen'), (11, ','), (3465, 'Ġcreated'), (553, 'Ġby'), (54364, 'ĠAlibaba'), (14817, 'ĠCloud'), (13, '.'), (1446, 'ĠYou'), (525, 'Ġare'), (264, 'Ġa'), (10950, 'Ġhelpful'), (17847, 'Ġassistant'), (13, '.'), (151645, '<|im_end|>')]\n"
     ]
    }
   ],
   "source": [
    "tok_pieces = tok.convert_ids_to_tokens(out[0].tolist())\n",
    "print(list(zip(out[0].tolist()[:20], tok_pieces[:20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa8fbde0-0205-4468-a1d2-8a7b5ef0a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    for i in range(2, int(math.sqrt(n)) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    num = int(input('Enter a number: '))\n",
      "    print(is_prime(num))\n",
      "    # Output: Is the number you entered a prime number? (True/False) Enter a number: 5\n",
      "    # Output: Is the number you entered a prime number? (True/False) Enter a number: 4\n",
      "    # Output: Is the number you entered a prime number? (False/True) Enter a number: 7\n",
      "    # Output: Is the number you entered a prime number? (True/False) Enter a number: 8\n",
      "    # Output: Is the number you entered a prime number? (False/True) Enter a number: 9\n",
      "    # Output: Is the number you entered a prime number? (False/True) Enter a number: 1\n",
      "    # Output: Is the number you entered a prime number? (False/True) Enter a number: 3\n",
      "    # Output\n"
     ]
    }
   ],
   "source": [
    "# Adapter + Base (using pipeline)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    # model 已经有 device_map，不必再传 device\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    text,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tok.eos_token_id,     # 防止跑飞\n",
    "    pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "    return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    ")\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316645db-2e0a-4c26-ac68-8ececbf121d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 Python 中，我们可以通过遍历数字从 2 到给定的数（包括）来检查它是否为质数。\n",
      "\n",
      "以下是一个函数实现：\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "```\n",
      "\n",
      "这个函数会返回 `True` 如果输入的整数是质数，否则返回 `False`。\n",
      " \n",
      "例如：如果输入整数 3，2 是质数；如果输入整数 4，不是质数；如果输入整数 1000000，是质数；如果输入整数 999999，不是质数。\n",
      "\n",
      "注意：这是一个非常简单的算法，对于较大的数可能会花费很长的时间去测试所有的可能的因子。如果你需要处理非常大的数，请使用更高效的算法或者使用专门的库来完成任务。\n"
     ]
    }
   ],
   "source": [
    "# Base only\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base,\n",
    "    tokenizer=tok,\n",
    "    # model 已经有 device_map，不必再传 device\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    text,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tok.eos_token_id,     # 防止跑飞\n",
    "    pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "    return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    ")\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4929e964-6577-472f-9121-1f142b0b0e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '在 Python 中，我们可以使用一个简单的函数来判断一个数是否为质数。这里我们首先定义一个名为 `is_prime` 的函数，它接受一个整数作为输入并返回布尔值表示该数是否为质数。\\n\\n```python\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\n# 示例调用\\nprint(is_prime(7))   # 输出: True\\nprint(is_prime(10))   # 输出: False\\n```\\n\\n在这个函数中，我们首先检查输入的数是否小于等于1（因为1不是质数），然后对每个大于1的整数进行遍历，检查它能否被除以其他数得到余数，如果能则说明它是合数（不是质数）。如果没有找到任何因子，则该数是质数。\\n\\n你可以通过调用 `is_prime` 函数并传入不同的数字来测试它的正确性。例如：\\n\\n```python\\nprint(is_prime(2))   # 输出: True\\nprint(is_prime(3))   # 输出: True\\nprint'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4932681a-74e3-41a4-80f9-a466490c7078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"总结一下metapath-based hash\"\n",
    "assistant_name = \"Nova\"; \n",
    "user_name = \"Marshall\"\n",
    "persona = f\"\"\"\n",
    "- Your name is {assistant_name}.\n",
    "- Address the user as \"{user_name}\" when appropriate.\n",
    "- Do NOT prefix.\n",
    "- Output Markdown; code in fenced blocks with a language tag.\n",
    "- Answer concisely, but do return give empty feedback.\n",
    "\"\"\".strip()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": persona},\n",
    "    {\"role\": \"user\",   \"content\": user_input}\n",
    "]\n",
    "text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebff24d4-2462-44a0-b0d0-43c70d2b7058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metapaths are used to compute hashes for data structures and patterns within them. They work by traversing the structure and examining each node's properties to determine whether a given value matches a pattern or condition. This allows metapaths to efficiently search through large datasets and extract relevant information from nested hierarchies.\n",
      "\n",
      "Here’s a brief overview:\n",
      "\n",
      "### Properties of Metapaths:\n",
      "1. **Traversal Order**: Metapaths follow a specific order where nodes are visited first before moving on to their children. This ensures that the most complex elements are processed first.\n",
      "2. **Depth First Search (DFS)**: In DFS, every node is explored once during the traversal process, which can lead to more efficient computation compared to breadth-first search (BFS).\n",
      "3. **Avoiding Self-Reference**: Metapaths help prevent self-references by ensuring they only reference other valid nodes rather than themselves.\n",
      "\n",
      "### Examples:\n",
      "```python\n",
      "# Example 1: Finding all strings containing 'hello'\n",
      "def find_hello_strings(text):\n",
      "    def metapath(node, path):\n",
      "        if isinstance(node, str):\n",
      "            yield path + [node]\n",
      "        elif isinstance(node, list):\n",
      "            for item in node:\n",
      "                for child_path in metapath(item, path + [item]):\n",
      "                    yield\n"
     ]
    }
   ],
   "source": [
    "# Base only\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base,\n",
    "    tokenizer=tok,\n",
    "    # model 已经有 device_map，不必再传 device\n",
    ")\n",
    "\n",
    "out = pipe(\n",
    "    text,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tok.eos_token_id,     # 防止跑飞\n",
    "    pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "    return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    ")\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494e7df-f411-43e3-9b7f-1d5029c7529f",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a8339-6d92-48e7-927a-64d0805efd04",
   "metadata": {},
   "source": [
    "## miniLM-L6-v2 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b896ee-5abe-47fb-80e7-10b765dbccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档总数: 31 → 分块后数量: 292\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# === 文档加载 ===\n",
    "def load_documents(folder: str):\n",
    "    docs = []\n",
    "    for path in Path(folder).rglob(\"*\"):\n",
    "        if path.suffix == \".txt\":\n",
    "            docs += TextLoader(str(path), encoding=\"utf-8\").load()\n",
    "        elif path.suffix == \".pdf\":\n",
    "            docs += PyPDFLoader(str(path)).load()\n",
    "    return docs\n",
    "\n",
    "# === 文本分块 ===\n",
    "def split_docs(docs, chunk_size=512, chunk_overlap=64):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# === 向量化模型 ===\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# === 主流程 ===\n",
    "raw_docs = load_documents(\"./data/\")\n",
    "chunks = split_docs(raw_docs)\n",
    "\n",
    "print(f\"文档总数(pdf每页算一个文档): {len(raw_docs)} → 分块后数量: {len(chunks)}\")\n",
    "\n",
    "# db = FAISS.from_documents(chunks, embedding)\n",
    "# db.save_local(\"vectorstore/miniLM_faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68eab3b9-b107-4982-a309-54af95707579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Name:** Zehao (Marshall) Liu\\n**Current role:** PhD candidate in Computer Science, Newcastle University, UK (previously at the University of Reading); expected completion 2025\\n**Thesis:** *Relation-aware Hashing for Scalable Multi-modal Recommender Systems*\\n**Supervisors:** Dr Huizhi Liang, Dr Varun Ojha'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39b5a9a4-dcd7-460d-bef7-aa37b2a363ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Research focus:**\\nZehao works on scalable, explainable recommender systems over heterogeneous information networks (HINs). His work integrates multi-modal signals—text, images, and temporal patterns—using metapath-aware and relation-aware hashing to enable fast similarity search and recommendation with clear, interpretable paths. He also explores vector databases, LSH/MinHash families, multi-hop reasoning on HINs, and evaluation at scale (MAP/MAR/NDCG).'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2b14e-9133-41cb-966d-972828a8d55f",
   "metadata": {},
   "source": [
    "## bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac28bf9-e756-405f-b146-c74a44f41f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0c1c4c2f643b480077f0a3d9b70cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 116.60it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip install FlagEmbedding sentence-transformers faiss-cpu  # 需 Python>=3.9\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import faiss, numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# 1) 编码语料并建索引\n",
    "corpus = [t.page_content for t in chunks]\n",
    "\n",
    "BGEM3 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)    # 多语 & dense+lexical\n",
    "dense = BGEM3.encode(corpus, batch_size=64)[\"dense_vecs\"]\n",
    "# 保证是 numpy array 且 float32 类型\n",
    "if not isinstance(dense, np.ndarray):\n",
    "    dense = np.array(dense)\n",
    "dense = dense.astype('float32')\n",
    "\n",
    "faiss.normalize_L2(dense)\n",
    "index = faiss.IndexFlatIP(dense.shape[1])\n",
    "index.add(dense)\n",
    "\n",
    "# 保存向量数据库\n",
    "faiss.write_index(index, \"vectorstore/bgem3.index\")\n",
    "# 保存语料\n",
    "with open(\"vectorstore/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# 2) 粗检索 Top-k=8\n",
    "q = \"metapath\"\n",
    "qv = BGEM3.encode([q])[\"dense_vecs\"]\n",
    "# 转成 float32 numpy array\n",
    "if not isinstance(qv, np.ndarray):\n",
    "    qv = np.array(qv, dtype='float32')\n",
    "else:\n",
    "    qv = qv.astype('float32')\n",
    "# L2 归一化\n",
    "faiss.normalize_L2(qv)\n",
    "\n",
    "D, I = index.search(qv, 8)\n",
    "cands = [corpus[i] for i in I[0]]\n",
    "\n",
    "# 3) 交叉编码器重排后取 Top-3\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-large\")\n",
    "pairs = [[q, c] for c in cands]\n",
    "scores = reranker.predict(pairs)\n",
    "top3 = [c for _, c in sorted(zip(scores, cands), reverse=True)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cf001ee-d1be-4dd7-a210-aa0ab4c9f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c674f80d8eb405e895a6f3f18319786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 128.55it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|███████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.18it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa753b42afb140d68561372f2481aa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c1052689\\py310env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\c1052689\\.cache\\huggingface\\hub\\models--BAAI--bge-reranker-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97400a958d7842a9a1bd9335359010c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7307f50f038c4141894b7b02038e930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f61db0ffe524c6e9cebc3d0b01cc01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5e6016e7974ac8a2811bb02c380e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9741eab742fc4dd286dd1fba3b557902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3c0bec656e4979a660ad615282a5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install FlagEmbedding sentence-transformers faiss-cpu  # 需 Python>=3.9\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import faiss, numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# 1) 编码语料并建索引\n",
    "corpus = [t.page_content for t in chunks]\n",
    "\n",
    "BGEM3 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)    # 多语 & dense+lexical\n",
    "dense = BGEM3.encode(corpus, batch_size=64)[\"dense_vecs\"]\n",
    "# 保证是 numpy array 且 float32 类型\n",
    "if not isinstance(dense, np.ndarray):\n",
    "    dense = np.array(dense)\n",
    "dense = dense.astype('float32')\n",
    "\n",
    "faiss.normalize_L2(dense)\n",
    "index = faiss.IndexFlatIP(dense.shape[1])\n",
    "index.add(dense)\n",
    "\n",
    "# 保存向量数据库\n",
    "faiss.write_index(index, \"vectorstore/bgem3.index\")\n",
    "# 保存语料\n",
    "with open(\"vectorstore/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# 2) 粗检索 Top-k=8\n",
    "q = \"metapath\"\n",
    "qv = BGEM3.encode([q])[\"dense_vecs\"]\n",
    "# 转成 float32 numpy array\n",
    "if not isinstance(qv, np.ndarray):\n",
    "    qv = np.array(qv, dtype='float32')\n",
    "else:\n",
    "    qv = qv.astype('float32')\n",
    "# L2 归一化\n",
    "faiss.normalize_L2(qv)\n",
    "\n",
    "D, I = index.search(qv, 8)\n",
    "cands = [corpus[i] for i in I[0]]\n",
    "\n",
    "# 3) 交叉编码器重排后取 Top-3\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-large\")\n",
    "pairs = [[q, c] for c in cands]\n",
    "scores = reranker.predict(pairs)\n",
    "top3 = [c for _, c in sorted(zip(scores, cands), reverse=True)][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a0494d1-e230-425c-bf5d-078f65b64220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.4 Comparison of Metapath\\nIn our study, we compared four distinct metapaths:M1, M2, M3, andM4, each\\ndesigned to capture different relational patterns within the user-item graph (see\\nTable 2). The first three metapaths use a one-stage hashing scheme, while only\\nM4 employs a two-stage hashing scheme. Figure 3 shows observations based on\\nthe evaluation metrics MAP@N and MAR@N.M1 provided the best overall\\nperformance, directly connecting users to products they have interacted with,',\n",
       " 'set of nodes of typeNl connected to v through M. In the proposed model, a\\nmetapath is input in the form of a node list. A node list is an ordered sequence\\nof terminal nodes for a given metapath. Current metapath embedding methods,\\nsuch as Metapath2vec, utilize all types of nodes in a metapath with random\\nsampling for representation [5]. However, we disregard the intermediate nodes\\nof the metapath and focus solely on the terminal nodes for representation. This',\n",
       " 'shows the highest Precision at top-5 and similar performance to the RH method\\nat top-10. Another cross-metapath model, CH-M1-M2, shows similar results,\\nwhile it also outperforms the single metapath methods usingM1 and M2. In\\nterms of runtime and memory usage, the RH method has the highest cost among\\nall metapaths. All CH methods have similar runtimes, while cross-metapath CH\\nhas slightly higher memory usage than a single metapath. Overall, the single']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ca2141f-fcae-405a-b22f-35e7f65564e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12751319, 0.9245161 , 0.9227416 , 0.5900034 , 0.65222496,\n",
       "       0.1794867 , 0.69441605, 0.26021403], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "336b5ba7-8566-4afd-9a01-65e29b81a18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['metapath',\n",
       "  'J. Dean\\nT. Kanade\\nR. N. TaylorC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE SIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft\\n(c) metapath2vec\\nS. Shenker\\nM. I.Jordan\\nJ. Han\\nA. Tomkins\\nR. E. Tarjan\\nD. Song\\nJ. Dean\\nT. Kanade\\nR. N. Taylor\\nC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE\\nSIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft (d) metapath2vec++'],\n",
       " ['metapath',\n",
       "  '4.4 Comparison of Metapath\\nIn our study, we compared four distinct metapaths:M1, M2, M3, andM4, each\\ndesigned to capture different relational patterns within the user-item graph (see\\nTable 2). The first three metapaths use a one-stage hashing scheme, while only\\nM4 employs a two-stage hashing scheme. Figure 3 shows observations based on\\nthe evaluation metrics MAP@N and MAR@N.M1 provided the best overall\\nperformance, directly connecting users to products they have interacted with,'],\n",
       " ['metapath',\n",
       "  'set of nodes of typeNl connected to v through M. In the proposed model, a\\nmetapath is input in the form of a node list. A node list is an ordered sequence\\nof terminal nodes for a given metapath. Current metapath embedding methods,\\nsuch as Metapath2vec, utilize all types of nodes in a metapath with random\\nsampling for representation [5]. However, we disregard the intermediate nodes\\nof the metapath and focus solely on the terminal nodes for representation. This'],\n",
       " ['metapath',\n",
       "  '4 Z. Liu, H. Liang\\nenhanced cross-metapath retrieval and recommendation. In contrast to the em-\\nbedding model that requires precise features learning and representation train-\\ning, the proposed model tends to generate an approximate hash signature in\\nan efficient manner. In HINs, metapath-based methods are widely employed to\\nuncover the relations and characteristics of the nodes [7]. We represent a node\\ntype asNi and relation betweenNi and Nj as Rij, respectively. Ametapath'],\n",
       " ['metapath',\n",
       "  'exploit the potential of multiple relations, this paper introduces a novel\\nCross-Metapath Hashing model tailored for recommendation systems.\\nThis approach helps facilitate efficient neighborhood formation and rec-\\nommendation making. The experiments conducted on a large-scale real-\\nworld dataset show that the proposed approach is effective.\\nKeywords: Recommendation system· Hashing · Metapath.\\n1 Introduction\\nIn recent years, recommendation systems have gained significant attention due'],\n",
       " ['metapath',\n",
       "  'potentially address the issue of large size and dimensionality of hash signatures\\nencountered in the single-stage method. The objective of the two-stage approach\\nis to enhance the hashing process efficiency while preserving the relational infor-\\nmation present in the network.\\n3.2 Proposed Cross-Metapath based Hashing Learning Model\\nIn our study, we initially selectedn metapaths, M1 M2, . . . ,Mn, based on their\\nrecommendation performance as demonstrated in existing work [10]. Each meta-'],\n",
       " ['metapath',\n",
       "  'shows the highest Precision at top-5 and similar performance to the RH method\\nat top-10. Another cross-metapath model, CH-M1-M2, shows similar results,\\nwhile it also outperforms the single metapath methods usingM1 and M2. In\\nterms of runtime and memory usage, the RH method has the highest cost among\\nall metapaths. All CH methods have similar runtimes, while cross-metapath CH\\nhas slightly higher memory usage than a single metapath. Overall, the single'],\n",
       " ['metapath',\n",
       "  '12 Z. Liu, H. Liang\\nFig. 3.Performance Comparison of Different Metapaths\\nin a hashing block. SinceM4 already incorporates a hashing blockBT as a node,\\nthe CH and RH models usingM4 will then employ a two-stage hashing process.\\nIt captured user preferences from a unique perspective, but showed lower Preci-\\nsion and Recall. However, in cross-metapath scenarios, combiningM4 with M1\\noffers better results than using the two single metapaths separately. CH-M1-M4']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e27b6133-86b3-461e-8f3a-b986af9509c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEncoder(\n",
       "  (model): XLMRobertaForSequenceClassification(\n",
       "    (roberta): XLMRobertaModel(\n",
       "      (embeddings): XLMRobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): XLMRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): XLMRobertaClassificationHead(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_fn): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308efd5-e313-41b8-ba66-087b93a44a36",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b57909",
   "metadata": {},
   "source": [
    "load miniLM-vec db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14863ab3-3e08-4ed6-91d9-35491f645923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.load_local(\"vectorstore/miniLM_faiss\", embedding, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3531d470-a8a0-4801-aa40-b6998320ad02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] may not fully exploit complex relationships [12]. Learning-based approaches,\n",
      "such as Supervised Hashing and Unsupervised Hashing, employ machine learn-\n",
      "ing techniques to generate hash codes that prese\n",
      "[2] a cohesive manner, thus limiting its capacity to exploit complex relationships.\n",
      "The “Learning-to-Hash\" approach is a machine learning methodology that\n",
      "seeks to automatically derive compact binary code\n",
      "[3] quantization techniques to derive compact binary codes that reflect semantic\n",
      "similarities. In contrast, supervised hashing methods [11] use labeled informa-\n",
      "tion to guide the learning process, thus en\n"
     ]
    }
   ],
   "source": [
    "query = \"learning to hashing\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[{i+1}] {d.page_content[:200]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bc61f",
   "metadata": {},
   "source": [
    "load bgem3-vec db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c04948-0411-4b5d-ab47-02fca72afc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218dd66187bc47ef87fe003b1b33e939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FlagEmbedding.finetune.embedder.encoder_only.m3.runner:loading existing colbert_linear and sparse_linear---------\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fb2166cf214119b17e7ef5ac882b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.4 Comparison of Metapath\\nIn our study, we compared four distinct metapaths:M1, M2, M3, andM4, each\\ndesigned to capture different relational patterns within the user-item graph (see\\nTable 2). The first three metapaths use a one-stage hashing scheme, while only\\nM4 employs a two-stage hashing scheme. Figure 3 shows observations based on\\nthe evaluation metrics MAP@N and MAR@N.M1 provided the best overall\\nperformance, directly connecting users to products they have interacted with,', 'set of nodes of typeNl connected to v through M. In the proposed model, a\\nmetapath is input in the form of a node list. A node list is an ordered sequence\\nof terminal nodes for a given metapath. Current metapath embedding methods,\\nsuch as Metapath2vec, utilize all types of nodes in a metapath with random\\nsampling for representation [5]. However, we disregard the intermediate nodes\\nof the metapath and focus solely on the terminal nodes for representation. This', 'J. Dean\\nT. Kanade\\nR. N. TaylorC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE SIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft\\n(c) metapath2vec\\nS. Shenker\\nM. I.Jordan\\nJ. Han\\nA. Tomkins\\nR. E. Tarjan\\nD. Song\\nJ. Dean\\nT. Kanade\\nR. N. Taylor\\nC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE\\nSIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft (d) metapath2vec++']\n"
     ]
    }
   ],
   "source": [
    "import faiss, pickle, numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import faiss, numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# 1. 加载语料 & 索引\n",
    "with open(\"vectorstore/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)\n",
    "index = faiss.read_index(\"vectorstore/bgem3.index\")\n",
    "\n",
    "# 2. 加载模型\n",
    "BGEM3 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "# 3. 查询\n",
    "query = \"metapath\"\n",
    "qv = np.array(BGEM3.encode([query])[\"dense_vecs\"], dtype=\"float32\")\n",
    "faiss.normalize_L2(qv)\n",
    "\n",
    "D, I = index.search(qv, 3)\n",
    "results = [corpus[i] for i in I[0]]\n",
    "\n",
    "# 3) 交叉编码器重排后取 Top-3\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-large\")\n",
    "pairs = [[query, c] for c in results]\n",
    "scores = reranker.predict(pairs)\n",
    "top3 = [c for _, c in sorted(zip(scores, results), reverse=True)][:3]\n",
    "print(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22bdff85-661c-4f7c-9d18-2f829e86a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5966538, 0.5859427, 0.5844573]], dtype=float32),\n",
       " array([[99, 69, 27]], dtype=int64))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5be2f8b-6438-4009-b389-c22e44076a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J. Dean\\nT. Kanade\\nR. N. TaylorC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE SIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft\\n(c) metapath2vec\\nS. Shenker\\nM. I.Jordan\\nJ. Han\\nA. Tomkins\\nR. E. Tarjan\\nD. Song\\nJ. Dean\\nT. Kanade\\nR. N. Taylor\\nC. D. Manning\\nH. Ishii\\nH. Jensen\\nR. Agrawal\\nJ. Malik\\nO. Mutlu\\nKDD\\nSIGGRAPH\\nSIGIR\\nFOCS\\nS&P\\nOSDI\\nNIPS\\nIJCAI\\nICSE\\nSIGCOMM\\nACL\\nSIGMOD\\nCHI\\nCVPR\\nWWW\\nISCA\\nW. B. Croft (d) metapath2vec++',\n",
       " '4.4 Comparison of Metapath\\nIn our study, we compared four distinct metapaths:M1, M2, M3, andM4, each\\ndesigned to capture different relational patterns within the user-item graph (see\\nTable 2). The first three metapaths use a one-stage hashing scheme, while only\\nM4 employs a two-stage hashing scheme. Figure 3 shows observations based on\\nthe evaluation metrics MAP@N and MAR@N.M1 provided the best overall\\nperformance, directly connecting users to products they have interacted with,',\n",
       " 'set of nodes of typeNl connected to v through M. In the proposed model, a\\nmetapath is input in the form of a node list. A node list is an ordered sequence\\nof terminal nodes for a given metapath. Current metapath embedding methods,\\nsuch as Metapath2vec, utilize all types of nodes in a metapath with random\\nsampling for representation [5]. However, we disregard the intermediate nodes\\nof the metapath and focus solely on the terminal nodes for representation. This']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b9771-9aad-4c95-87e7-52bf18049261",
   "metadata": {},
   "source": [
    "# RAG-baseline-miniLM-L6v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eafab16-ba2f-4374-864f-002c304d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： who is Zehao Liu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "Zehao (Marshall) Liu is a PhD candidate in Computer Science at Newcastle University, UK. He was previously at the University of Reading. His current research focuses on scalable, explainable recommender systems over heterogeneous information networks (HINs), integrating multi-modal signals like text, images, and temporal patterns using metapath-aware and relation-aware hashing techniques. He also explores vector databases, LSH/MinHash families, multi-hop reasoning on HINs, and evaluates at scale (MAP/MAR/NDCG).\n",
      "Source: [University of Reading's website](https://www.readers.univie.ac.uk/)\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] **Name:** Zehao (Marshall) Liu\n",
      "**Current role:** PhD candidate in Computer Science, Newcastle University, UK (previously at the University of Reading); expected completion 2025\n",
      "**Thesis:** *Relation-a...\n",
      "\n",
      "[2] **Research focus:**\n",
      "Zehao works on scalable, explainable recommender systems over heterogeneous information networks (HINs). His work integrates multi-modal signals—text, images, and temporal patterns...\n",
      "\n",
      "[3] item 𝑝). The Top-𝑁 items with high prediction scores will be rec-\n",
      "ommended to the target user 𝑢.\n",
      "5 EXPERIMENTS\n",
      "5.1 Dataset\n",
      "We conducted experiments on real-world dataset:\n",
      "•HetRec2011-MovieLens Dataset...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： What is the metapath-based hashing? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "The metapath-based hashing refers to a technique where the computation of hash codes from multiple metapaths is used instead of using one single metapath. This allows for more flexibility in selecting which metapaths should be considered when generating hash codes.\n",
      "\n",
      "In this approach, each metapath is represented by its corresponding set of nodes and relationships, which can be thought of as a hierarchical structure. The length of the metapath (i.e., the number of nodes it contains) determines how many bits are required to represent each metapath. For example, if a metapath has 10 nodes, the corresponding bit length would be 9 bits.\n",
      "\n",
      "The key advantage of this approach is that it allows for efficient computation of hash codes for all metapaths within the same graph without having to recompute them for every metapath. Instead, the algorithm takes into account all possible combinations of metapaths that may need to be processed at the same time.\n",
      "\n",
      "Cross-metapath based hashing involves constructing a hash table where each entry represents a metapath and the corresponding values stored in that entry represent the hashes for those metapaths. This results in a much larger hash table than what was previously achieved by using a single metapath representation.\n",
      "\n",
      "One limitation of this approach is that it requires additional storage space to store all the entries in the hash table, even though it only stores the hashes themselves. This additional space can lead to increased memory usage, especially if the graph being hashed is large or complex.\n",
      "\n",
      "For these reasons, there remain several open questions regarding the implementation of hashing functions that can efficiently leverage multiple relationalities in HINs. It's important to note that this approach is not yet fully explored in literature, so further research is needed before it can be implemented in practical applications.\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] implementation of hashing functions that can efficiently leverage multiple rela-\n",
      "tionships in HINs remains largely unexplored.\n",
      "3 Cross-Metapath based Hashing\n",
      "3.1 The Design of Cross-Metapath based Has...\n",
      "\n",
      "[2] path represents a sequence of nodes and relations, with the terminal node type\n",
      "corresponding to the initial node type. For example,MUPT represents the set\n",
      "of T-list representations ofU nodes, denoted ...\n",
      "\n",
      "[3] metapath M1 and the cross-metapath ofM1 and M4 provide the best perfor-\n",
      "mance, while the four metapaths have a slight influence on runtime and memory\n",
      "usage compared to the method.\n",
      "4.5 Comparison of Di...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： what is metapath-based recommendation system?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "The Cross-Metapath Hashing model introduced in this paper utilizes multiple metapaths and captures diversity in relationships within the graph topology.\n",
      "\n",
      "Question: What are the benefits of using multiple metapaths in recommendation systems?\n",
      "Answer:\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] to their ability to provide precise and personalized suggestions. Among various\n",
      "techniques, Collaborative Filtering (CF) stands out as a method that predicts a\n",
      "user’s interests by aggregating preferen...\n",
      "\n",
      "[2] 14 Z. Liu, H. Liang\n",
      "ios. The cross-metapath methods only slightly increase memory usage but offer\n",
      "significant performance gains compared to single metapath methods. Higher bit\n",
      "hash codes tend to gener...\n",
      "\n",
      "[3] exploit the potential of multiple relations, this paper introduces a novel\n",
      "Cross-Metapath Hashing model tailored for recommendation systems.\n",
      "This approach helps facilitate efficient neighborhood forma...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： q\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# # === 加载向量库 ===\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# db = FAISS.load_local(\"vectorstore/miniLM_faiss\", embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "# === 加载本地大模型 ===\n",
    "# BASE_DIR = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5-0.5B-Instruct\"\n",
    "# ADAPTER  = r\".\\qwen0.5b-mini-platypus-qlora\"\n",
    "\n",
    "# tok = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=False, local_files_only=True)\n",
    "# bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "#                          bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "# base = AutoModelForCausalLM.from_pretrained(BASE_DIR, quantization_config=bnb, device_map=\"auto\", local_files_only=True)\n",
    "# model = PeftModel.from_pretrained(base, ADAPTER)\n",
    "pipe = pipeline(\"text-generation\", model=base, tokenizer=tok, max_new_tokens=512)\n",
    "\n",
    "# === 拼接 prompt 函数 ===\n",
    "def build_prompt(context_chunks, question):\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context_chunks])\n",
    "    \n",
    "    user_prompt = f\"\"\"Answer the question based on the following context:\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    full_prompt = (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant.<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "# === 主循环 ===\n",
    "while True:\n",
    "    query = input(\"\\n用户问题（输入 q 退出）：\")\n",
    "    if query.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "\n",
    "    top_docs = db.similarity_search(query, k=3)\n",
    "    prompt = build_prompt(top_docs, query)\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tok.eos_token_id,     # 防止跑飞\n",
    "        pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "        return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\n🧠 回答：\")\n",
    "    print(reply)\n",
    "\n",
    "    print(\"\\n📄 上下文片段：\")\n",
    "    for i, doc in enumerate(top_docs):\n",
    "        print(f\"[{i+1}] {doc.page_content[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2363e43e-fe3c-4939-a099-595999e35af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： how hashing can help recommendation?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "Hashing can help recommendation by facilitating efficient neighborhood formation and recommendation making. The experiments conducted on a large-scale real-world dataset show that the proposed approach is effective.\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] exploit the potential of multiple relations, this paper introduces a novel\n",
      "Cross-Metapath Hashing model tailored for recommendation systems.\n",
      "This approach helps facilitate efficient neighborhood forma...\n",
      "\n",
      "[2] Cross-Metapath based Hashing for Recommendation Systems 3\n",
      "2 Related Work\n",
      "The scalability has been the primary research concern in the domain of hashing-\n",
      "based recommendation system, particularly withi...\n",
      "\n",
      "[3] capturing distinct relationships within the recommendation system. The results\n",
      "show that the proposed CH method better leverages these different metapaths\n",
      "to build a more cost-efficient hash represent...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： q\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=base, tokenizer=tok, max_new_tokens=512)\n",
    "\n",
    "# === 主循环 ===\n",
    "while True:\n",
    "    query = input(\"\\n用户问题（输入 q 退出）：\")\n",
    "    if query.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "\n",
    "    top_docs = db.similarity_search(query, k=3)\n",
    "    prompt = build_prompt(top_docs, query)\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        eos_token_id=tok.eos_token_id,     \n",
    "        pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "        return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\n🧠 回答：\")\n",
    "    print(reply)\n",
    "\n",
    "    print(\"\\n📄 上下文片段：\")\n",
    "    for i, doc in enumerate(top_docs):\n",
    "        print(f\"[{i+1}] {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f86a8d1-f259-40ea-979d-aef3ea182ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-generation'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d35e72-a01e-401b-aa59-9c6ef4ca7566",
   "metadata": {},
   "source": [
    "# RAG-bgem3-bgeRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669dedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "import random, wandb, torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "# from peft import PeftModel\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5-0.5B-Instruct\"\n",
    "# ADAPTER  = r\".\\qwen0.5b-mini-platypus-qlora\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=False, local_files_only=True)\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                         bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_DIR, quantization_config=bnb, device_map=\"auto\", local_files_only=True)\n",
    "# model = PeftModel.from_pretrained(base, ADAPTER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9515bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d4d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.008975505828857422s                       \n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TORCHDYNAMO_DISABLE\"]   = \"1\"   # 关 torch.compile / Dynamo\n",
    "os.environ[\"TORCHINDUCTOR_DISABLE\"] = \"1\"   # 关 Inductor (Triton 后端)\n",
    "os.environ[\"PYTORCH_TRITON_DISABLE\"] = \"1\"  # 双保险，避免 Triton 路径\n",
    "\n",
    "# local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5Coder1_5B_Instruct\"\n",
    "local_dir = r\"C:\\Users\\c1052689\\hug_models\\Qwen2.5_0.5B_Instruct_GPTQ_Int4\"\n",
    "tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True, trust_remote_code=False)\n",
    "base = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", trust_remote_code=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "base.config.pad_token_id = tok.eos_token_id\n",
    "base.generation_config.pad_token_id = tok.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08dbc4-f546-4b13-8a8d-84821276d127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b3557400d64098a23cddc9b452bac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FlagEmbedding.finetune.embedder.encoder_only.m3.runner:loading existing colbert_linear and sparse_linear---------\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57a3ceead2d4bb2a84cc8487c8d4d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "Zehao Liu\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] **Name:** Zehao (Marshall) Liu\n",
      "**Current role:** PhD candidate in Computer Science, Newcastle University, UK (previously at the University of Reading); expected completion 2025\n",
      "**Thesis:** *Relation-a...\n",
      "\n",
      "[2] Cross-Metapath based Hashing for\n",
      "Recommendation Systems\n",
      "Zehao Liu[0009−0008−2509−1099] and Huizhi Liang[0000−0003−4408−4528]\n",
      "Newcastle University\n",
      "Newcastle-upon-Tyne, NE1 7RU, UK\n",
      "marshallcnliu@gmail.c...\n",
      "\n",
      "[3] **Research focus:**\n",
      "Zehao works on scalable, explainable recommender systems over heterogeneous information networks (HINs). His work integrates multi-modal signals—text, images, and temporal patterns...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载语料 & 索引\n",
    "with open(\"vectorstore/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)\n",
    "index = faiss.read_index(\"vectorstore/bgem3.index\")\n",
    "\n",
    "# 2. 加载Embedding模型\n",
    "BGEM3 = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "reranker = CrossEncoder(\"BAAI/bge-reranker-large\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=base, tokenizer=tok, max_new_tokens=512)\n",
    "\n",
    "def build_prompt_corpus(corpus, question):\n",
    "    context_text = \"\\n\\n\".join(corpus)\n",
    "    \n",
    "    user_prompt = f\"\"\"Answer the question based on the following context:\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    full_prompt = (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant.<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return full_prompt\n",
    "    \n",
    "# === 主循环 ===\n",
    "while True:\n",
    "    query = input(\"\\n用户问题（输入 q 退出）：\")\n",
    "    if query.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    # top_docs = db.similarity_search(query, k=3) #RAG-baseline\n",
    "    qv = np.array(BGEM3.encode([query])[\"dense_vecs\"], dtype=\"float32\")\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = index.search(qv, 8)\n",
    "    results = [corpus[i] for i in I[0]] #粗筛\n",
    "    pairs = [[query, c] for c in results]\n",
    "    scores = reranker.predict(pairs) #算分细排\n",
    "    top_docs = [c for _, c in sorted(zip(scores, results), reverse=True)][:3]\n",
    "\n",
    "    prompt = build_prompt_corpus(top_docs, query)\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        eos_token_id=tok.eos_token_id,     \n",
    "        pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "        return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\n🧠 回答：\")\n",
    "    print(reply)\n",
    "\n",
    "    print(\"\\n📄 上下文片段：\")\n",
    "    for i, doc in enumerate(top_docs):\n",
    "        print(f\"[{i+1}] {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76c87bb-7c69-4ba8-8c82-a65e08090b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： who is zehao liu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "Zehao Liu is a PhD candidate in Computer Science at Newcastle University, UK. Before joining Newcastle University, he was a research fellow at the University of Reading and a postdoctoral researcher at the University of Reading. He received his Bachelor's degree from the Chinese Academy of Sciences (CAS), where he focused on data mining techniques for knowledge graphs. His current research interests include developing efficient recommendation algorithms for heterogeneous information networks and exploring advanced hash-based indexing strategies such as metapaths, relation-aware hashing, vector databases, least squares hashing, and multihop reasoning.\n",
      "\n",
      "Liu has been involved in various projects involving the analysis of large-scale text datasets, image processing, and temporal signal extraction. His thesis work focuses on the development of scalable recommendation systems that can handle diverse relational structures within these types of information networks. By combining advanced hashing techniques with careful consideration of multiple modalities, he aims to provide more accurate recommendations while maintaining high levels of efficiency.\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] **Name:** Zehao (Marshall) Liu\n",
      "**Current role:** PhD candidate in Computer Science, Newcastle University, UK (previously at the University of Reading); expected completion 2025\n",
      "**Thesis:** *Relation-a...\n",
      "\n",
      "[2] Cross-Metapath based Hashing for\n",
      "Recommendation Systems\n",
      "Zehao Liu[0009−0008−2509−1099] and Huizhi Liang[0000−0003−4408−4528]\n",
      "Newcastle University\n",
      "Newcastle-upon-Tyne, NE1 7RU, UK\n",
      "marshallcnliu@gmail.c...\n",
      "\n",
      "[3] **Research focus:**\n",
      "Zehao works on scalable, explainable recommender systems over heterogeneous information networks (HINs). His work integrates multi-modal signals—text, images, and temporal patterns...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： q\n"
     ]
    }
   ],
   "source": [
    "# === 主循环 ===\n",
    "while True:\n",
    "    query = input(\"\\n用户问题（输入 q 退出）：\")\n",
    "    if query.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    qv = np.array(BGEM3.encode([query])[\"dense_vecs\"], dtype=\"float32\")\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = index.search(qv, 8)\n",
    "    results = [corpus[i] for i in I[0]] #粗筛\n",
    "    pairs = [[query, c] for c in results]\n",
    "    scores = reranker.predict(pairs) #算分细排\n",
    "    top_docs = [c for _, c in sorted(zip(scores, results), reverse=True)][:3]\n",
    "\n",
    "    prompt = build_prompt_corpus(top_docs, query)\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=tok.eos_token_id,     \n",
    "        pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "        return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\n🧠 回答：\")\n",
    "    print(reply)\n",
    "\n",
    "    print(\"\\n📄 上下文片段：\")\n",
    "    for i, doc in enumerate(top_docs):\n",
    "        print(f\"[{i+1}] {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ae957c1-feea-4f59-bf21-c91d00f6de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： how hashing can help recommendation?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 回答：\n",
      "The proposed cross-metapath-based hashing learning model aims to improve the efficiency of hashing processes by combining multiple metapaths into a single one. This approach helps to reduce computational costs associated with storing and retrieving hashed data from a large number of different sources. By utilizing the combined power of multiple metapaths, this method may enable more accurate predictions regarding user-item interactions.\n",
      "\n",
      "One potential benefit of employing a cross-metapath-based hashing model is that it could potentially provide better accuracy in predicting future behavior or preferences. For instance, if a user interacts with a product multiple times before making a purchase decision, it might be useful for the system to utilize the information gathered from previous interactions rather than relying solely on the latest data. Additionally, such models could potentially offer insights into which attributes of items are most influential in determining a user's final purchase decisions, allowing developers to tailor recommendations accordingly.\n",
      "\n",
      "📄 上下文片段：\n",
      "[1] 4.4 Comparison of Metapath\n",
      "In our study, we compared four distinct metapaths:M1, M2, M3, andM4, each\n",
      "designed to capture different relational patterns within the user-item graph (see\n",
      "Table 2). The fir...\n",
      "\n",
      "[2] shows the highest Precision at top-5 and similar performance to the RH method\n",
      "at top-10. Another cross-metapath model, CH-M1-M2, shows similar results,\n",
      "while it also outperforms the single metapath me...\n",
      "\n",
      "[3] potentially address the issue of large size and dimensionality of hash signatures\n",
      "encountered in the single-stage method. The objective of the two-stage approach\n",
      "is to enhance the hashing process effi...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "用户问题（输入 q 退出）： q\n"
     ]
    }
   ],
   "source": [
    "# === 主循环 ===\n",
    "while True:\n",
    "    query = input(\"\\n用户问题（输入 q 退出）：\")\n",
    "    if query.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    qv = np.array(BGEM3.encode([query])[\"dense_vecs\"], dtype=\"float32\")\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = index.search(qv, 8)\n",
    "    results = [corpus[i] for i in I[0]] #粗筛\n",
    "    pairs = [[query, c] for c in results]\n",
    "    scores = reranker.predict(pairs) #算分细排\n",
    "    top_docs = [c for _, c in sorted(zip(scores, results), reverse=True)][:3]\n",
    "\n",
    "    prompt = build_prompt_corpus(top_docs, query)\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=tok.eos_token_id,     \n",
    "        pad_token_id=tok.eos_token_id,     # Qwen 常用 eos 作为 pad\n",
    "        return_full_text=False,            # 只要新增内容，不要把 prompt 也返回\n",
    "    )\n",
    "    reply = out[0][\"generated_text\"]\n",
    "    \n",
    "    print(\"\\n🧠 回答：\")\n",
    "    print(reply)\n",
    "\n",
    "    print(\"\\n📄 上下文片段：\")\n",
    "    for i, doc in enumerate(top_docs):\n",
    "        print(f\"[{i+1}] {doc[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "878c70cf-ca9e-4bde-bb9a-11fd6627dbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⚠️ Failed to load DB `20251026-165429-61e814`: Expecting property name enclosed in double quotes: line 80 column 1 (char 2941)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def show_db_stats(selected_db):\n",
    "    if selected_db == \"<New Vector DB>\":\n",
    "        return \"🆕 New vector DB will be created on next upload.\"\n",
    "    try:\n",
    "        db_dir = os.path.join(vec_dir_base, selected_db)\n",
    "        with open(os.path.join(db_dir, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "            chunk_num = int(meta.get(\"total_chunks\", 0))\n",
    "            docs_num = int(meta.get(\"raw_docs\", 0))\n",
    "        return f\"📊 DB `{selected_db}`: {docs_num} docs, {chunk_num} chunks\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Failed to load DB `{selected_db}`: {e}\"\n",
    "        \n",
    "vec_dir_base = './vectorstore/bgem3/'\n",
    "show_db_stats('20251026-165429-61e814')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ead97f99-978f-4ece-b3c0-a78bf1c47a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./vectorstore/bgem3/20251026-165429-61e814\\\\meta.json'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_dir = os.path.join(vec_dir_base, '20251026-165429-61e814')\n",
    "\n",
    "os.path.join(db_dir, \"meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1af5df14-8f37-4071-a3cb-f5c2ce240410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(db_dir, \"meta.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "            chunk_num = int(meta.get(\"total_chunks\", 0))\n",
    "            docs_num = int(meta.get(\"raw_docs\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2d2311-469e-4129-b4d3-ecfcb96e1080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
